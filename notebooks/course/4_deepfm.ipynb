{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM: Factorization-Machines based Neural Network\n",
    "\n",
    "This notebook implements **DeepFM** from the paper [\"DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\"](https://arxiv.org/abs/1703.04247) (Guo et al., 2017) using PyTorch.\n",
    "\n",
    "## Overview\n",
    "\n",
    "DeepFM combines two powerful components that share the same input embeddings:\n",
    "\n",
    "1. **FM (Factorization Machine) Component**: Captures low-order (1st and 2nd order) feature interactions through factorized parameters.\n",
    "\n",
    "2. **Deep Component**: A deep neural network (MLP) that learns high-order feature interactions from the same embeddings.\n",
    "\n",
    "### Rich Features in This Implementation\n",
    "\n",
    "Unlike NeuMF which only uses user_id and item_id, this DeepFM implementation leverages the full MovieLens 100K dataset:\n",
    "\n",
    "**User Features:**\n",
    "- `user_id`: Unique user identifier\n",
    "- `age_bucket`: Age grouped into buckets (7 buckets)\n",
    "- `gender`: M/F (2 categories)\n",
    "- `occupation`: 21 different occupations\n",
    "\n",
    "**Item Features:**\n",
    "- `item_id`: Unique movie identifier\n",
    "- `genres`: 19 binary genre indicators (Action, Comedy, Drama, etc.)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                    Sparse Input Features                      │\n",
    "│  [user_id, age, gender, occupation, item_id, genre1, ..., g19]│\n",
    "└───────────────────────────┬──────────────────────────────────┘\n",
    "                            │\n",
    "              ┌─────────────▼─────────────┐\n",
    "              │   Shared Embedding Layer  │\n",
    "              │   (one embedding per field)│\n",
    "              └─────────────┬─────────────┘\n",
    "                            │\n",
    "          ┌─────────────────┴─────────────────┐\n",
    "          │                                   │\n",
    "┌─────────▼─────────┐             ┌───────────▼───────────┐\n",
    "│   FM Component    │             │    Deep Component     │\n",
    "│  1st + 2nd order  │             │   (MLP layers)        │\n",
    "│   interactions    │             │   Higher-order        │\n",
    "└─────────┬─────────┘             └───────────┬───────────┘\n",
    "          │                                   │\n",
    "          └─────────────────┬─────────────────┘\n",
    "                            │\n",
    "              ┌─────────────▼─────────────┐\n",
    "              │   Sigmoid(bias + FM + DNN)│\n",
    "              │   → ŷ (prediction)        │\n",
    "              └───────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 2025\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MovieLens 100K Dataset with Rich Features\n",
    "\n",
    "We load three files:\n",
    "- `u.data`: User-item interactions with ratings and timestamps\n",
    "- `u.user`: User demographics (age, gender, occupation, zip code)\n",
    "- `u.item`: Movie information (title, release date, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = \"/Users/codexplore/Developer/repos/recommender-system/data/movielens/ml-100k\"\n",
    "\n",
    "# Load interactions\n",
    "df = pd.read_csv(\n",
    "    f\"{DATA_DIR}/u.data\", \n",
    "    sep=\"\\t\", \n",
    "    names=[\"user\", \"item\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "print(f\"Total interactions: {len(df):,}\")\n",
    "print(f\"Unique users: {df['user'].nunique():,}\")\n",
    "print(f\"Unique items: {df['item'].nunique():,}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user features\n",
    "df_users = pd.read_csv(\n",
    "    f\"{DATA_DIR}/u.user\",\n",
    "    sep=\"|\",\n",
    "    names=[\"user\", \"age\", \"gender\", \"occupation\", \"zip_code\"],\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "print(f\"User features shape: {df_users.shape}\")\n",
    "print(f\"\\nAge range: {df_users['age'].min()} - {df_users['age'].max()}\")\n",
    "print(f\"Genders: {df_users['gender'].unique().tolist()}\")\n",
    "print(f\"Occupations ({df_users['occupation'].nunique()}): {df_users['occupation'].unique().tolist()}\")\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load item features (movies with genres)\n",
    "genre_names = [\n",
    "    \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n",
    "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
    "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]\n",
    "\n",
    "item_cols = [\"item\", \"title\", \"release_date\", \"video_release\", \"url\"] + genre_names\n",
    "\n",
    "df_items = pd.read_csv(\n",
    "    f\"{DATA_DIR}/u.item\",\n",
    "    sep=\"|\",\n",
    "    names=item_cols,\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "print(f\"Item features shape: {df_items.shape}\")\n",
    "print(f\"\\nGenre columns: {genre_names}\")\n",
    "print(f\"\\nExample movie genres:\")\n",
    "df_items[[\"title\"] + genre_names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We process the features to create categorical indices for DeepFM:\n",
    "\n",
    "1. **User features**: \n",
    "   - Age → bucketed into 7 groups\n",
    "   - Gender → 2 categories (M=0, F=1)\n",
    "   - Occupation → 21 categories\n",
    "\n",
    "2. **Item features**:\n",
    "   - 19 genre fields, each binary (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age buckets\n",
    "age_bins = [0, 18, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = list(range(len(age_bins) - 1))  # 0-6\n",
    "df_users[\"age_bucket\"] = pd.cut(df_users[\"age\"], bins=age_bins, labels=age_labels, right=False).astype(int)\n",
    "\n",
    "print(\"Age bucket distribution:\")\n",
    "print(df_users[\"age_bucket\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "gender2id = {\"M\": 0, \"F\": 1}\n",
    "df_users[\"gender_id\"] = df_users[\"gender\"].map(gender2id)\n",
    "\n",
    "occupation2id = {occ: i for i, occ in enumerate(df_users[\"occupation\"].unique())}\n",
    "df_users[\"occupation_id\"] = df_users[\"occupation\"].map(occupation2id)\n",
    "\n",
    "print(f\"Number of genders: {len(gender2id)}\")\n",
    "print(f\"Number of occupations: {len(occupation2id)}\")\n",
    "print(f\"\\nOccupation mapping: {occupation2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to implicit feedback\n",
    "df[\"interaction\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "df_implicit = df[df[\"interaction\"] == 1].copy()\n",
    "\n",
    "print(f\"Positive interactions (rating >= 4): {len(df_implicit):,}\")\n",
    "print(f\"Sparsity: {1 - len(df_implicit) / (df['user'].nunique() * df['item'].nunique()):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 0-based user and item indices\n",
    "user2id = {u: i for i, u in enumerate(df_implicit[\"user\"].unique())}\n",
    "item2id = {i: j for j, i in enumerate(df_implicit[\"item\"].unique())}\n",
    "id2user = {i: u for u, i in user2id.items()}\n",
    "id2item = {j: i for i, j in item2id.items()}\n",
    "\n",
    "df_implicit[\"uid\"] = df_implicit[\"user\"].map(user2id)\n",
    "df_implicit[\"iid\"] = df_implicit[\"item\"].map(item2id)\n",
    "\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "\n",
    "print(f\"Number of users: {n_users}\")\n",
    "print(f\"Number of items: {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build user feature lookup: uid -> (age_bucket, gender_id, occupation_id)\n",
    "user_features = {}\n",
    "for _, row in df_users.iterrows():\n",
    "    original_user_id = row[\"user\"]\n",
    "    if original_user_id in user2id:\n",
    "        uid = user2id[original_user_id]\n",
    "        user_features[uid] = (\n",
    "            int(row[\"age_bucket\"]),\n",
    "            int(row[\"gender_id\"]),\n",
    "            int(row[\"occupation_id\"])\n",
    "        )\n",
    "\n",
    "print(f\"User features loaded for {len(user_features)} users\")\n",
    "print(f\"Example user 0 features (age_bucket, gender, occupation): {user_features[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build item feature lookup: iid -> [genre1, genre2, ..., genre19]\n",
    "# Each genre is binary (0 or 1)\n",
    "item_features = {}\n",
    "for _, row in df_items.iterrows():\n",
    "    original_item_id = row[\"item\"]\n",
    "    if original_item_id in item2id:\n",
    "        iid = item2id[original_item_id]\n",
    "        # Get genre values (19 binary features)\n",
    "        genres = [int(row[g]) for g in genre_names]\n",
    "        item_features[iid] = genres\n",
    "\n",
    "print(f\"Item features loaded for {len(item_features)} items\")\n",
    "print(f\"Example item 0 genres: {item_features[0]}\")\n",
    "print(f\"Genre names: {genre_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define field dimensions for DeepFM\n",
    "# Each field has its own vocabulary size\n",
    "\n",
    "n_age_buckets = len(age_labels)  # 7\n",
    "n_genders = len(gender2id)  # 2\n",
    "n_occupations = len(occupation2id)  # 21\n",
    "n_genres = len(genre_names)  # 19 (each is binary: 0 or 1)\n",
    "\n",
    "# Field dimensions: [user_id, age, gender, occupation, item_id, genre1, genre2, ..., genre19]\n",
    "# For binary genre fields, dimension is 2 (0 or 1)\n",
    "field_dims = (\n",
    "    [n_users, n_age_buckets, n_genders, n_occupations, n_items] + \n",
    "    [2] * n_genres  # 19 binary genre fields\n",
    ")\n",
    "\n",
    "n_fields = len(field_dims)\n",
    "\n",
    "print(f\"Number of feature fields: {n_fields}\")\n",
    "print(f\"Field dimensions: {field_dims}\")\n",
    "print(f\"Total feature vocabulary: {sum(field_dims):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Validation/Test Split\n",
    "\n",
    "We use **temporal leave-one-out** splitting:\n",
    "- For each user, sort interactions by timestamp\n",
    "- Last interaction → Test set\n",
    "- Second-to-last interaction → Validation set  \n",
    "- All other interactions → Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "# User interaction history for negative sampling\n",
    "user_pos_items = defaultdict(set)\n",
    "\n",
    "for uid, hist in df_implicit.groupby(\"uid\"):\n",
    "    hist = hist.sort_values(\"timestamp\")\n",
    "    items = hist[\"iid\"].tolist()\n",
    "    \n",
    "    if len(items) < 3:\n",
    "        continue\n",
    "    \n",
    "    user_pos_items[uid] = set(items)\n",
    "    \n",
    "    train_items = items[:-2]\n",
    "    val_item = items[-2]\n",
    "    test_item = items[-1]\n",
    "    \n",
    "    for iid in train_items:\n",
    "        train_data.append((uid, iid))\n",
    "    val_data.append((uid, val_item))\n",
    "    test_data.append((uid, test_item))\n",
    "\n",
    "print(f\"Train interactions: {len(train_data):,}\")\n",
    "print(f\"Validation users: {len(val_data):,}\")\n",
    "print(f\"Test users: {len(test_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training interaction matrix\n",
    "train_df = pd.DataFrame(train_data, columns=[\"uid\", \"iid\"])\n",
    "\n",
    "R_train = csr_matrix(\n",
    "    (np.ones(len(train_df)), (train_df[\"uid\"], train_df[\"iid\"])),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "# Create user -> training items mapping\n",
    "user_train_items = defaultdict(set)\n",
    "for uid, iid in train_data:\n",
    "    user_train_items[uid].add(iid)\n",
    "\n",
    "# Validation and test ground truth\n",
    "val_ground_truth = {uid: {iid} for uid, iid in val_data}\n",
    "test_ground_truth = {uid: {iid} for uid, iid in test_data}\n",
    "\n",
    "print(f\"Training matrix shape: {R_train.shape}\")\n",
    "print(f\"Training matrix density: {R_train.nnz / (R_train.shape[0] * R_train.shape[1]):.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset with Rich Features\n",
    "\n",
    "The dataset now returns feature vectors containing:\n",
    "- User features: `[user_id, age_bucket, gender, occupation]`\n",
    "- Item features: `[item_id, genre1, genre2, ..., genre19]`\n",
    "\n",
    "Total: 24 feature fields per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFMTrainDataset(Dataset):\n",
    "    \"\"\"Dataset for DeepFM training with rich features and negative sampling.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data: List[Tuple[int, int]],\n",
    "        user_pos_items: Dict[int, Set[int]],\n",
    "        user_features: Dict[int, Tuple],\n",
    "        item_features: Dict[int, List[int]],\n",
    "        n_items: int,\n",
    "        n_negatives: int = 4\n",
    "    ):\n",
    "        self.train_data = train_data\n",
    "        self.user_pos_items = user_pos_items\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        self.n_items = n_items\n",
    "        self.n_negatives = n_negatives\n",
    "        \n",
    "        # Get list of valid item ids (those with features)\n",
    "        self.valid_items = list(item_features.keys())\n",
    "        \n",
    "        self._build_samples()\n",
    "    \n",
    "    def _build_feature_vector(self, uid: int, iid: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build feature vector for a user-item pair.\n",
    "        \n",
    "        Returns: [user_id, age, gender, occupation, item_id, g1, g2, ..., g19]\n",
    "        \"\"\"\n",
    "        # User features\n",
    "        user_feat = self.user_features.get(uid, (0, 0, 0))\n",
    "        age, gender, occupation = user_feat\n",
    "        \n",
    "        # Item features (genres)\n",
    "        item_genres = self.item_features.get(iid, [0] * 19)\n",
    "        \n",
    "        # Combine: [user_id, age, gender, occupation, item_id, genres...]\n",
    "        return [uid, age, gender, occupation, iid] + item_genres\n",
    "    \n",
    "    def _build_samples(self):\n",
    "        \"\"\"Build training samples with negative sampling.\"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        for uid, pos_iid in self.train_data:\n",
    "            # Skip if item doesn't have features\n",
    "            if pos_iid not in self.item_features:\n",
    "                continue\n",
    "                \n",
    "            # Positive sample\n",
    "            features = self._build_feature_vector(uid, pos_iid)\n",
    "            self.samples.append((features, 1.0))\n",
    "            \n",
    "            # Negative samples\n",
    "            neg_count = 0\n",
    "            while neg_count < self.n_negatives:\n",
    "                neg_iid = np.random.choice(self.valid_items)\n",
    "                if neg_iid not in self.user_pos_items[uid]:\n",
    "                    features = self._build_feature_vector(uid, neg_iid)\n",
    "                    self.samples.append((features, 0.0))\n",
    "                    neg_count += 1\n",
    "    \n",
    "    def refresh_negatives(self):\n",
    "        \"\"\"Resample negatives for a new epoch.\"\"\"\n",
    "        self._build_samples()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(features, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFMEvalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DeepFM evaluation with rich features.\n",
    "    \n",
    "    Each sample returns a user's features combined with ALL items for scoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        user_ids: List[int],\n",
    "        n_items: int,\n",
    "        ground_truth: Dict[int, Set[int]],\n",
    "        user_train_items: Dict[int, Set[int]],\n",
    "        user_features: Dict[int, Tuple],\n",
    "        item_features: Dict[int, List[int]]\n",
    "    ):\n",
    "        self.user_ids = user_ids\n",
    "        self.n_items = n_items\n",
    "        self.ground_truth = ground_truth\n",
    "        self.user_train_items = user_train_items\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        \n",
    "        # Pre-compute item feature matrix: (n_items, 20) - [item_id, genres...]\n",
    "        self.item_feature_matrix = []\n",
    "        for iid in range(n_items):\n",
    "            genres = item_features.get(iid, [0] * 19)\n",
    "            self.item_feature_matrix.append([iid] + genres)\n",
    "        self.item_feature_matrix = torch.tensor(self.item_feature_matrix, dtype=torch.long)\n",
    "        \n",
    "        # Pre-compute masks for training items\n",
    "        self.train_masks = {}\n",
    "        for uid in user_ids:\n",
    "            mask = torch.zeros(n_items, dtype=torch.bool)\n",
    "            train_items = user_train_items.get(uid, set())\n",
    "            if train_items:\n",
    "                mask[list(train_items)] = True\n",
    "            self.train_masks[uid] = mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.user_ids[idx]\n",
    "        \n",
    "        # User features: [age, gender, occupation] repeated for all items\n",
    "        user_feat = self.user_features.get(uid, (0, 0, 0))\n",
    "        age, gender, occupation = user_feat\n",
    "        \n",
    "        # Build feature matrix: (n_items, 24)\n",
    "        # [user_id, age, gender, occupation, item_id, g1, ..., g19]\n",
    "        user_feats = torch.tensor(\n",
    "            [[uid, age, gender, occupation]] * self.n_items, \n",
    "            dtype=torch.long\n",
    "        )  # (n_items, 4)\n",
    "        \n",
    "        # Concatenate user features with item features\n",
    "        features = torch.cat([user_feats, self.item_feature_matrix], dim=1)  # (n_items, 24)\n",
    "        \n",
    "        # Ground truth\n",
    "        gt_items = self.ground_truth.get(uid, set())\n",
    "        gt_tensor = torch.zeros(self.n_items, dtype=torch.bool)\n",
    "        if gt_items:\n",
    "            gt_tensor[list(gt_items)] = True\n",
    "        \n",
    "        # Training mask\n",
    "        train_mask = self.train_masks[uid]\n",
    "        \n",
    "        return features, gt_tensor, train_mask, uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "n_negatives = 4\n",
    "\n",
    "train_dataset = DeepFMTrainDataset(\n",
    "    train_data, user_pos_items, user_features, item_features, n_items, n_negatives\n",
    ")\n",
    "print(f\"Training samples (with {n_negatives} negatives per positive): {len(train_dataset):,}\")\n",
    "\n",
    "# Validation and test datasets\n",
    "val_user_ids = list(val_ground_truth.keys())\n",
    "test_user_ids = list(test_ground_truth.keys())\n",
    "\n",
    "val_dataset = DeepFMEvalDataset(\n",
    "    val_user_ids, n_items, val_ground_truth, user_train_items, user_features, item_features\n",
    ")\n",
    "test_dataset = DeepFMEvalDataset(\n",
    "    test_user_ids, n_items, test_ground_truth, user_train_items, user_features, item_features\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset: {len(val_dataset)} users\")\n",
    "print(f\"Test dataset: {len(test_dataset)} users\")\n",
    "\n",
    "# Verify feature vector shape\n",
    "sample_features, sample_label = train_dataset[0]\n",
    "print(f\"\\nFeature vector shape: {sample_features.shape}\")\n",
    "print(f\"Feature vector: {sample_features.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DeepFM Model Architecture\n",
    "\n",
    "The model now handles 24 feature fields:\n",
    "- 4 user fields: user_id, age_bucket, gender, occupation\n",
    "- 20 item fields: item_id + 19 genre indicators\n",
    "\n",
    "Each field gets its own embedding, and all embeddings are shared between FM and Deep components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorization Machine Layer with shared embeddings.\n",
    "    \n",
    "    Computes:\n",
    "    - First-order: sum of linear weights for each feature\n",
    "    - Second-order: pairwise feature interactions via factorized embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First-order weights\n",
    "        self.first_order = nn.Embedding(n_features, 1)\n",
    "        \n",
    "        # Second-order embeddings (shared with Deep component)\n",
    "        self.embeddings = nn.Embedding(n_features, embed_dim)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.normal_(self.first_order.weight, std=0.01)\n",
    "        nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, feature_indices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_indices: (batch_size, n_fields) global feature indices\n",
    "            \n",
    "        Returns:\n",
    "            fm_output: (batch_size, 1) FM prediction\n",
    "            embeddings: (batch_size, n_fields, embed_dim) for Deep component\n",
    "        \"\"\"\n",
    "        # First-order\n",
    "        first_order_out = self.first_order(feature_indices).sum(dim=1)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeds = self.embeddings(feature_indices)\n",
    "        \n",
    "        # Second-order: 0.5 * (sum^2 - sum of squares)\n",
    "        sum_of_embeds = embeds.sum(dim=1)\n",
    "        sum_of_squares = (embeds ** 2).sum(dim=1)\n",
    "        square_of_sum = sum_of_embeds ** 2\n",
    "        \n",
    "        second_order_out = 0.5 * (square_of_sum - sum_of_squares).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        fm_out = first_order_out + second_order_out\n",
    "        \n",
    "        return fm_out, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepComponent(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Component: MLP on concatenated embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fields: int,\n",
    "        embed_dim: int,\n",
    "        hidden_layers: List[int],\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = n_fields * embed_dim\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = embeddings.shape[0]\n",
    "        flat_embeds = embeddings.view(batch_size, -1)\n",
    "        return self.mlp(flat_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepFM with rich features.\n",
    "    \n",
    "    Handles multiple feature fields with different vocabulary sizes.\n",
    "    Uses shared embeddings between FM and Deep components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        field_dims: List[int],\n",
    "        embed_dim: int = 32,\n",
    "        hidden_layers: List[int] = [64, 32, 16],\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.field_dims = field_dims\n",
    "        self.n_fields = len(field_dims)\n",
    "        \n",
    "        # Field offsets for global indexing\n",
    "        self.register_buffer(\n",
    "            'offsets',\n",
    "            torch.tensor([0] + list(np.cumsum(field_dims)[:-1]), dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        n_features = sum(field_dims)\n",
    "        \n",
    "        # Global bias\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # FM Layer\n",
    "        self.fm = FMLayer(n_features, embed_dim)\n",
    "        \n",
    "        # Deep Component\n",
    "        self.deep = DeepComponent(\n",
    "            n_fields=self.n_fields,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_layers=hidden_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, feature_indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_indices: (batch_size, n_fields) field-local indices\n",
    "            \n",
    "        Returns:\n",
    "            (batch_size,) prediction scores\n",
    "        \"\"\"\n",
    "        # Convert to global indices\n",
    "        global_indices = feature_indices + self.offsets.unsqueeze(0)\n",
    "        \n",
    "        # FM component\n",
    "        fm_out, embeddings = self.fm(global_indices)\n",
    "        \n",
    "        # Deep component\n",
    "        deep_out = self.deep(embeddings)\n",
    "        \n",
    "        # Combine\n",
    "        logits = self.bias + fm_out + deep_out\n",
    "        \n",
    "        return torch.sigmoid(logits).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    \"embed_dim\": 16,  # Smaller embedding since we have more fields\n",
    "    \"hidden_layers\": [128, 64, 32],  # Larger network for more features\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 20,\n",
    "    \"weight_decay\": 1e-5\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = DeepFM(\n",
    "    field_dims=field_dims,\n",
    "    embed_dim=config[\"embed_dim\"],\n",
    "    hidden_layers=config[\"hidden_layers\"],\n",
    "    dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nNumber of fields: {n_fields}\")\n",
    "print(f\"Field dimensions: {field_dims}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(rec_items: List[int], true_items: Set[int], K: int) -> float:\n",
    "    if not true_items:\n",
    "        return 0.0\n",
    "    n_hit = len(set(rec_items[:K]) & true_items)\n",
    "    return n_hit / len(true_items)\n",
    "\n",
    "\n",
    "def hr_at_k(rec_items: List[int], true_items: Set[int], K: int) -> float:\n",
    "    return 1.0 if len(set(rec_items[:K]) & true_items) > 0 else 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(rec_items: List[int], true_items: Set[int], K: int) -> float:\n",
    "    dcg = sum(\n",
    "        1.0 / np.log2(i + 2) \n",
    "        for i, item in enumerate(rec_items[:K]) \n",
    "        if item in true_items\n",
    "    )\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(true_items), K)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def map_at_k(rec_items: List[int], true_items: Set[int], K: int) -> float:\n",
    "    if not true_items:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    sum_precision = 0.0\n",
    "    for i, item in enumerate(rec_items[:K]):\n",
    "        if item in true_items:\n",
    "            hits += 1\n",
    "            sum_precision += hits / (i + 1)\n",
    "    return sum_precision / min(len(true_items), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    eval_dataset: DeepFMEvalDataset,\n",
    "    K: int = 10,\n",
    "    batch_size: int = 32  # Smaller batch due to larger feature vectors\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model with batched inference.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    recalls, hit_rates, ndcgs, maps = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features_batch, gt_batch, mask_batch, uids in eval_loader:\n",
    "            batch_len = features_batch.shape[0]\n",
    "            n_items = features_batch.shape[1]\n",
    "            \n",
    "            # Flatten: (batch_size * n_items, n_fields)\n",
    "            features_flat = features_batch.reshape(-1, features_batch.shape[-1]).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            scores = model(features_flat)\n",
    "            scores = scores.reshape(batch_len, n_items).cpu()\n",
    "            \n",
    "            # Mask training items\n",
    "            scores[mask_batch] = float('-inf')\n",
    "            \n",
    "            # Top-K\n",
    "            top_k_indices = torch.topk(scores, K, dim=1).indices\n",
    "            \n",
    "            # Metrics\n",
    "            for i in range(batch_len):\n",
    "                rec_items = top_k_indices[i].tolist()\n",
    "                true_items = set(torch.where(gt_batch[i])[0].tolist())\n",
    "                \n",
    "                recalls.append(recall_at_k(rec_items, true_items, K))\n",
    "                hit_rates.append(hr_at_k(rec_items, true_items, K))\n",
    "                ndcgs.append(ndcg_at_k(rec_items, true_items, K))\n",
    "                maps.append(map_at_k(rec_items, true_items, K))\n",
    "    \n",
    "    return {\n",
    "        \"Recall@K\": np.mean(recalls),\n",
    "        \"HitRate@K\": np.mean(hit_rates),\n",
    "        \"NDCG@K\": np.mean(ndcgs),\n",
    "        \"MAP@K\": np.mean(maps)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepfm(\n",
    "    model: nn.Module,\n",
    "    train_dataset: DeepFMTrainDataset,\n",
    "    val_dataset: DeepFMEvalDataset,\n",
    "    config: dict,\n",
    "    early_stopping_patience: int = 5\n",
    ") -> Tuple[nn.Module, List[dict]]:\n",
    "    \"\"\"Train DeepFM with early stopping.\"\"\"\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config[\"lr\"], \n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    history = []\n",
    "    best_ndcg = 0.0\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Validation\n",
    "        K = 10\n",
    "        val_metrics = evaluate_model(model, val_dataset, K=K, batch_size=32)\n",
    "        val_ndcg = val_metrics[\"NDCG@K\"]\n",
    "        \n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": avg_loss,\n",
    "            \"time\": epoch_time,\n",
    "            **val_metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{config['epochs']} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"Val NDCG@{K}: {val_ndcg:.4f} | \"\n",
    "              f\"Val HR@{K}: {val_metrics['HitRate@K']:.4f} | \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            best_epoch = epoch + 1\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}. Best epoch: {best_epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Resample negatives\n",
    "        train_dataset.refresh_negatives()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config[\"batch_size\"], \n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\nLoaded best model from epoch {best_epoch} (NDCG@10: {best_ndcg:.4f})\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training with rich features...\\n\")\n",
    "print(f\"Features: user_id, age_bucket, gender, occupation, item_id, + 19 genres\")\n",
    "print(f\"Total fields: {n_fields}\\n\")\n",
    "\n",
    "model, history = train_deepfm(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=config,\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "\n",
    "test_metrics = {}\n",
    "for K in [5, 10, 20]:\n",
    "    metrics = evaluate_model(model, test_dataset, K=K, batch_size=32)\n",
    "    test_metrics[K] = metrics\n",
    "\n",
    "results_df = pd.DataFrame(test_metrics).T\n",
    "results_df.index.name = \"K\"\n",
    "print(\"Test Set Results (DeepFM with Rich Features):\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history_df[\"epoch\"], history_df[\"loss\"], marker=\"o\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"BCE Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_df[\"epoch\"], history_df[\"NDCG@K\"], marker=\"o\", color=\"green\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"NDCG@10\")\n",
    "axes[1].set_title(\"Validation NDCG@10\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history_df[\"epoch\"], history_df[\"HitRate@K\"], marker=\"o\", color=\"orange\")\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Hit Rate@10\")\n",
    "axes[2].set_title(\"Validation Hit Rate@10\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison\n",
    "\n",
    "Compare DeepFM with rich features against NeuMF (user_id + item_id only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference results from NeuMF\n",
    "neumf_results = {\n",
    "    5: {\"Recall@K\": 0.0764, \"HitRate@K\": 0.0764, \"NDCG@K\": 0.0486, \"MAP@K\": 0.0395},\n",
    "    10: {\"Recall@K\": 0.1465, \"HitRate@K\": 0.1465, \"NDCG@K\": 0.0710, \"MAP@K\": 0.0486},\n",
    "    20: {\"Recall@K\": 0.2367, \"HitRate@K\": 0.2367, \"NDCG@K\": 0.0940, \"MAP@K\": 0.0550},\n",
    "}\n",
    "\n",
    "print(\"Model Comparison (Test Set)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"NeuMF: user_id + item_id (2 fields)\")\n",
    "print(f\"DeepFM: user_id + age + gender + occupation + item_id + 19 genres ({n_fields} fields)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for K in [5, 10, 20]:\n",
    "    print(f\"\\n@K = {K}:\")\n",
    "    print(f\"  {'Metric':<12} {'NeuMF':>10} {'DeepFM':>10} {'Diff':>10} {'% Change':>10}\")\n",
    "    print(f\"  {'-'*54}\")\n",
    "    for metric in [\"Recall@K\", \"NDCG@K\", \"MAP@K\"]:\n",
    "        neumf_val = neumf_results[K][metric]\n",
    "        deepfm_val = test_metrics[K][metric]\n",
    "        diff = deepfm_val - neumf_val\n",
    "        pct_change = (diff / neumf_val * 100) if neumf_val != 0 else 0\n",
    "        sign = \"+\" if diff >= 0 else \"\"\n",
    "        print(f\"  {metric:<12} {neumf_val:>10.4f} {deepfm_val:>10.4f} {sign}{diff:>9.4f} {sign}{pct_change:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Analysis\n",
    "\n",
    "Let's analyze what the model learned about different feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding norms by feature field\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get all embeddings\n",
    "    all_embeds = model.fm.embeddings.weight.cpu().numpy()\n",
    "    \n",
    "    # Compute field-wise statistics\n",
    "    field_names = [\n",
    "        \"user_id\", \"age\", \"gender\", \"occupation\", \"item_id\"\n",
    "    ] + [f\"genre_{g}\" for g in genre_names]\n",
    "    \n",
    "    offsets = [0] + list(np.cumsum(field_dims))\n",
    "    \n",
    "    print(\"Embedding Statistics by Field:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Field':<20} {'Size':>8} {'Mean Norm':>12} {'Std':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (name, start, end) in enumerate(zip(field_names, offsets[:-1], offsets[1:])):\n",
    "        field_embeds = all_embeds[start:end]\n",
    "        norms = np.linalg.norm(field_embeds, axis=1)\n",
    "        print(f\"{name:<20} {end-start:>8} {np.mean(norms):>12.4f} {np.std(norms):>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize genre importance (by first-order weights)\n",
    "with torch.no_grad():\n",
    "    first_order_weights = model.fm.first_order.weight.cpu().numpy().squeeze()\n",
    "    \n",
    "    # Extract genre weights (fields 5-23)\n",
    "    genre_start = sum(field_dims[:5])  # After user_id, age, gender, occupation, item_id\n",
    "    \n",
    "    genre_weights = []\n",
    "    for i, genre in enumerate(genre_names):\n",
    "        # Each genre field has 2 values (0 and 1)\n",
    "        field_start = genre_start + i * 2\n",
    "        # Weight for genre=1 minus weight for genre=0\n",
    "        weight_diff = first_order_weights[field_start + 1] - first_order_weights[field_start]\n",
    "        genre_weights.append((genre, weight_diff))\n",
    "    \n",
    "    # Sort by weight\n",
    "    genre_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nGenre First-Order Weights (positive = increases prediction):\")\n",
    "    print(\"=\" * 40)\n",
    "    for genre, weight in genre_weights:\n",
    "        bar = \"+\" * int(abs(weight) * 50) if weight > 0 else \"-\" * int(abs(weight) * 50)\n",
    "        print(f\"{genre:<15} {weight:>8.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Rich Features in DeepFM\n",
    "\n",
    "This implementation demonstrates DeepFM's full potential by incorporating:\n",
    "\n",
    "1. **User Features** (4 fields):\n",
    "   - `user_id`: Captures user-specific preferences\n",
    "   - `age_bucket`: Age-related preferences (7 buckets)\n",
    "   - `gender`: Gender-specific tastes\n",
    "   - `occupation`: Occupation-based patterns\n",
    "\n",
    "2. **Item Features** (20 fields):\n",
    "   - `item_id`: Movie-specific characteristics\n",
    "   - `19 genres`: Content-based signals (Action, Comedy, Drama, etc.)\n",
    "\n",
    "### Why Rich Features Matter\n",
    "\n",
    "- **Cold Start**: Genre features help recommend new movies without interaction history\n",
    "- **Cross-Feature Learning**: FM captures interactions like \"young males prefer Action movies\"\n",
    "- **Better Generalization**: User demographics provide signals beyond just interaction patterns\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "1. **Shared Embeddings**: Single embedding per feature shared between FM and Deep\n",
    "2. **Efficient FM**: O(n) pairwise interactions via sum-of-squares trick\n",
    "3. **Joint Training**: FM and DNN components optimize together\n",
    "\n",
    "### Comparison with NeuMF\n",
    "\n",
    "| Aspect | NeuMF | DeepFM (Rich Features) |\n",
    "|--------|-------|------------------------|\n",
    "| Feature fields | 2 | 24 |\n",
    "| User features | ID only | ID + demographics |\n",
    "| Item features | ID only | ID + genres |\n",
    "| Cross-features | None | FM captures all pairs |\n",
    "| Cold start | Poor | Better (content features) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
